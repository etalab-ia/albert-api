from typing import Any, Dict, List, Literal, Optional, Union

from openai.types.chat import ChatCompletion, ChatCompletionChunk
from pydantic import Field, field_validator, model_validator

from app.schemas import BaseModel
from app.schemas.search import Search, SearchArgs
from app.schemas.usage import Usage

DEFAULT_RAG_TEMPLATE = "Réponds à la question suivante en te basant sur les documents ci-dessous : {prompt}\n\nDocuments :\n{chunks}"


class ChatSearchArgs(SearchArgs):
    template: str = Field(
        description='Template to use for the RAG query. The template must contain "{chunks}" and "{prompt}" placeholders.',
        default=DEFAULT_RAG_TEMPLATE,
    )

    @field_validator("template")
    def validate_template(cls, value):
        if "{chunks}" not in value:
            raise ValueError('template must contain "{chunks}" placeholder')
        if "{prompt}" not in value:
            raise ValueError('template must contain "{prompt}" placeholder')

        return value


class ChatCompletionRequest(BaseModel):
    # only union between OpenAI fields and vLLM fields are defined. See https://github.com/vllm-project/vllm/blob/main/vllm/entrypoints/openai/protocol.py#L209
    messages: List = Field(description="A list of messages comprising the conversation so far.")  # fmt: off
    model: str = Field(description="ID of the model to use. Call `/v1/models` endpoint to get the list of available models, only `text-generation` model type is supported.")  # fmt: off
    frequency_penalty: Optional[float] = Field(default=0.0, description="Number between -2.0 and 2.0. Positive values penalize new tokens based on their existing frequency in the text so far, decreasing the model's likelihood to repeat the same line verbatim.")  # fmt: off
    logit_bias: Optional[Dict[str, float]] = Field(default=None, description="Modify the likelihood of specified tokens appearing in the completion. Accepts a JSON object that maps tokens (specified by their token ID in the tokenizer) to an associated bias value from -100 to 100. Mathematically, the bias is added to the logits generated by the model prior to sampling. The exact effect will vary per model, but values between -1 and 1 should decrease or increase likelihood of selection; values like -100 or 100 should result in a ban or exclusive selection of the relevant token.")  # fmt: off
    logprobs: Optional[bool] = Field(default=False, description="Whether to return log probabilities of the output tokens or not. If true, returns the log probabilities of each output token returned in the `content` of `message`.")  # fmt: off
    top_logprobs: Optional[int] = Field(default=None, description="An integer between 0 and 20 specifying the number of most likely tokens to return at each token position, each with an associated log probability. `logprobs` must be set to `true` if this parameter is used.")  # fmt: off
    presence_penalty: Optional[float] = Field(default=0.0, description="Number between -2.0 and 2.0. Positive values penalize new tokens based on whether they appear in the text so far, increasing the model's likelihood to talk about new topics.")  # fmt: off
    max_tokens: Optional[int] = Field(default=None, description="The maximum number of tokens that can be generated in the chat completion. This value can be used to control costs for text generated via API.", deprecated=True)  # fmt: off
    max_completion_tokens: Optional[int] = Field(default=None, description="An upper bound for the number of tokens that can be generated for a completion.")  # fmt: off
    n: Optional[int] = Field(default=1, description="How many chat completion choices to generate for each input message. Note that you will be charged based on the number of generated tokens across all of the choices. Keep `n` as `1` to minimize costs.")  # fmt: off
    response_format: Optional[Any] = Field(default=None, description="Setting to `{ \"type\": \"json_schema\", \"json_schema\": {...} }` enables Structured Outputs which ensures the model will match your supplied JSON schema. Learn more in the Structured Outputs guide. Setting to `{ \"type\": \"json_object\" }` enables JSON mode, which ensures the message the model generates is valid JSON.<br>**Important**: when using JSON mode, you must also instruct the model to produce JSON yourself via a system or user message. Without this, the model may generate an unending stream of whitespace until the generation reaches the token limit, resulting in a long-running and seemingly \"stuck\" request. Also note that the message content may be partially cut off if `finish_reason=\"length\"`, which indicates the generation exceeded `max_tokens` or the conversation exceeded the max context length.")  # fmt: off
    seed: Optional[int] = Field(default=None, description="If specified, our system will make a best effort to sample deterministically, such that repeated requests with the same `seed` and parameters should return the same result. Determinism is not guaranteed, and you should refer to the system_fingerprint response parameter to monitor changes in the backend.")  # fmt: off
    stop: Optional[Union[str, List[str]]] = Field(default_factory=list, description="Up to 4 sequences where the API will stop generating further tokens.")  # fmt: off
    stream: Optional[Literal[True, False]] = Field(default=False, description="If set, partial message deltas will be sent. Tokens will be sent as data-only server-sent events as they become available, with the stream terminated by a data: [DONE] message.")  # fmt: off
    stream_options: Optional[Any] = Field(default=None, description="Options for streaming response. Only set this when you set `stream: true`.")  # fmt: off
    temperature: Optional[float] = Field(default=0.7, description="What sampling temperature to use, between 0 and 2. Higher values like 0.8 will make the output more random, while lower values like 0.2 will make it more focused and deterministic. We generally recommend altering this or `top_p` but not both.")  # fmt: off
    top_p: Optional[float] = Field(default=1, description="An alternative to sampling with temperature, called nucleus sampling, where the model considers the results of the tokens with top_p probability mass. So 0.1 means only the tokens comprising the top 10% probability mass are considered.<br>We generally recommend altering this or `temperature` but not both.")  # fmt: off
    tools: Optional[List] = Field(default=None, description="A list of tools the model may call. Currently, only functions are supported as a tool. Use this to provide a list of functions the model may generate JSON inputs for.")  # fmt: off
    tool_choice: Any = Field(default="none", description="Controls which (if any) tool is called by the model. `none` means the model will not call any tool and instead generates a message. `auto` means the model can pick between generating a message or calling one or more tools. `required` means the model must call one or more tools. Specifying a particular tool via `{\"type\": \"function\", \"function\": {\"name\": \"my_function\"}}` forces the model to call that tool.<br>`none` is the default when no tools are present. `auto` is the default if tools are present.")  # fmt: off

    # search additional fields
    search: bool = Field(default=False)  # fmt: off
    search_args: Optional[ChatSearchArgs] = Field(default=None)  # fmt: off

    @model_validator(mode="after")
    def validate_model(cls, values):
        if values.search:
            if not values.search_args:
                raise ValueError("search_args is required when search is true")

        return values


class ChatCompletion(ChatCompletion):
    id: str = Field(default=None, description="A unique identifier for the chat completion.")
    search_results: List[Search] = []
    usage: Usage = Field(default_factory=Usage, description="Usage information for the request.")


class ChatCompletionChunk(ChatCompletionChunk):
    search_results: List[Search] = []
